{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNr+PDXC+F/WbxS4Sw5GB91",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Martin-Munive/RAG_MED/blob/main/RAG_MED_001.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.0. Instalación de librerías.\n"
      ],
      "metadata": {
        "id": "9yfvqgZr-ODF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Core y Modelos de Lenguaje ---\n",
        "!pip install -q -U google-generativeai #1\n",
        "!pip install -q -U langchain #2\n",
        "!pip install -q -U langchain_google_genai #3\n",
        "# --- Carga y Procesamiento de Documentos ---\n",
        "!pip install -q -U pypdfium2 #4\n",
        "# --- Base de Datos Vectorial y Componentes de la Comunidad ---\n",
        "!pip install -q -U chromadb #5\n",
        "!pip install -q -U langchain-community #6\n",
        "\n",
        "## DOCUMENTACIÓN:\n",
        "# 1. SDK oficial de Google para interactuar con los modelos de IAG.\n",
        "# 2. Framework principal para flujo del RAG.\n",
        "# 3. Paquete adaptador que permite a LangChain comunicarse con Gemini.\n",
        "# 4. Librería base para la manipulación y procesamiento de PDFs.\n",
        "# -- Permite la extracción del texto.\n",
        "# 5. Base de datos vectorial. Almacena los \"chunks\" de texto y sus vectores.\n",
        "# 6. Paquete de las integraciones y componentes.\n",
        "\n",
        "## PENDIENTE\n",
        "# La siguiente librería no es necesaria para el funcionamiento del RAG.\n",
        "# Se tiene en consideración para una futura optimización.\n",
        "# Re-ranking de resultados.\n",
        "# -- Mejora la precisión de la fase de recuperación.\n",
        "# !pip install -q -U flashrank\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zlr_Ggfu-gNM",
        "outputId": "f430f5f4-179b-441d-9bfb-777358310361"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m442.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.18 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.2/313.2 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m933.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m815.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.6/19.6 MB\u001b[0m \u001b[31m47.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m71.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.1/103.1 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.6/65.6 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.0/120.0 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.6/201.6 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.4/71.4 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m93.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m453.1/453.1 kB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.0. Configuración de la API-KEY."
      ],
      "metadata": {
        "id": "uPd10oeP_qBJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Importación de Módulos Necesarios ---\n",
        "import google.generativeai as genai #1\n",
        "from google.colab import userdata #2\n",
        "import os #3\n",
        "\n",
        "# DOCUMENTACIÓN\n",
        "# 1. SDK de Google: para la configuración directa de la API.\n",
        "# 2. Módulo de Colab: permite acceder a los \"Secretos\" guardados.\n",
        "# 3. Módulo del Sistema Operativo: para establecer variables de entorno.\n",
        "\n",
        "# Configuración Segura de la API Key de Google\n",
        "# Se gestiona el error durante su verificación.\n",
        "try:\n",
        "    GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "    os.environ['GOOGLE_API_KEY'] = GOOGLE_API_KEY\n",
        "    genai.configure(api_key=GOOGLE_API_KEY)\n",
        "    print(\"API Key configurada exitosamente desde los secretos de Colab.\")\n",
        "except userdata.SecretNotFoundError:\n",
        "    print(\"Error: El secreto 'GOOGLE_API_KEY' no fue encontrado.\")\n",
        "    print(\"Por favor, sigue las instrucciones para añadir el secreto usando el ícono de la llave (🔑) a la izquierda.\")\n",
        "except Exception as e:\n",
        "    print(f\"Ocurrió un error inesperado: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jcZOkVLn_wYZ",
        "outputId": "da447f9b-5786-42e9-a92f-31715e8e8c74"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API Key configurada exitosamente desde los secretos de Colab.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.0. Vinculación de Google Drive."
      ],
      "metadata": {
        "id": "ya0RF0KqFGzX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Importación del Módulo de Drive ---\n",
        "from google.colab import drive\n",
        "\n",
        "# --- Ejecución del Montaje ---\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "print(\"Conectado exitosamente a Google Drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_y-Y6Va1Fft9",
        "outputId": "3f6cb0fd-0bab-4fa1-ce7b-1dffe6d0716c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n",
            "Conectado exitosamente a Google Drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.0. Cargar y dividir los PDFs."
      ],
      "metadata": {
        "id": "jGpMN3ImHCpH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Importación de Módulos para Carga y Procesamiento de Texto ---\n",
        "import os #1\n",
        "from langchain_community.document_loaders import PyPDFium2Loader #2\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter #3\n",
        "\n",
        "# DOCUMENTACIÓN:\n",
        "# 1. Proporciona funciones para interactuar con el sistema operativo.\n",
        "# 2. Cargador de directorios de PDF. Escanear la carpeta y buscar archivos.\n",
        "# 3. Clase para dividir texto. Toma el texto de los PDF y los fragmenta.\n",
        "\n",
        "# Ruta a la carpeta en Google Drive\n",
        "DATA_PATH = \"/content/drive/MyDrive/RAG_MED-001_DB\"\n",
        "print(f\"Buscando archivos PDF en la carpeta: {DATA_PATH}\")\n",
        "\n",
        "# Carga para múltiples PDFs.\n",
        "all_documents = []\n",
        "\n",
        "# Itera sobre cada archivo en el directorio.\n",
        "# Verifica si el archivo es un pdf.\n",
        "# Se construye la ruta del archivo.\n",
        "for filename in os.listdir(DATA_PATH):\n",
        "    if filename.lower().endswith(\".pdf\"):\n",
        "        file_path = os.path.join(DATA_PATH, filename)\n",
        "        print(f\"Procesando archivo: {file_path}\")\n",
        "        # Se cargan los archivos PDF y se gestiona el error.\n",
        "        try:\n",
        "            loader = PyPDFium2Loader(file_path)\n",
        "            all_documents.extend(loader.load())\n",
        "        except Exception as e:\n",
        "            print(f\"Error al procesar el archivo {filename}: {e}\")\n",
        "\n",
        "\n",
        "# División de los documentos cargados.\n",
        "# Se gestiona el error y se renombra la lista.\n",
        "if not all_documents:\n",
        "    print(\"¡Advertencia! No se cargaron documentos. Verifica la ruta y el contenido de la carpeta.\")\n",
        "else:\n",
        "    documents = all_documents\n",
        "\n",
        "# Dividimos los documentos en chunks.\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
        "    docs = text_splitter.split_documents(documents)\n",
        "\n",
        "    print(f\"\\nSe han cargado un total de {len(documents)} página(s) desde los archivos PDF.\")\n",
        "    print(f\"Se han dividido en {len(docs)} fragmentos (chunks).\")\n",
        "    if docs:\n",
        "        print(\"Ejemplo del primer chunk:\", docs[0].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HYobG3gHSfs",
        "outputId": "db47bcfd-2714-42ae-a069-e36458dd686e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Buscando archivos PDF en la carpeta: /content/drive/MyDrive/RAG_MED-001_DB\n",
            "Procesando archivo: /content/drive/MyDrive/RAG_MED-001_DB/Gross-Anatomy-The-Big-Picture.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pypdfium2/_helpers/textpage.py:80: UserWarning: get_text_range() call with default params will be implicitly redirected to get_text_bounded()\n",
            "  warnings.warn(\"get_text_range() call with default params will be implicitly redirected to get_text_bounded()\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Se han cargado un total de 513 página(s) desde los archivos PDF.\n",
            "Se han dividido en 2844 fragmentos (chunks).\n",
            "Ejemplo del primer chunk: GROSS ANATOMY\n",
            "THE BIG PICTURE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1. Verificación de los chunks.\n",
        "\n"
      ],
      "metadata": {
        "id": "ayo7o64TZ8PF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imprime los primeros 5 chunks para inspeccionar su calidad.\n",
        "print(\"--- Verificando la calidad de los primeros 5 chunks ---\")\n",
        "for i, doc in enumerate(docs[:5]):\n",
        "    print(f\"--- CHUNK {i+1} (Página: {doc.metadata.get('page', 'N/A')}) ---\")\n",
        "    print(doc.page_content)\n",
        "    print(\"---------------------------------------------------\\n\")\n",
        "# Muestra exactamente cómo se está dividiendo el PDF.\n",
        "# Si hay títulos de sección, texto desordenado o contenido sin sentido.\n",
        "# --- Permite identificar problemas es la extracción del PDF.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2pZ8WGcaEde",
        "outputId": "01fed6ce-58f7-413f-95cc-d071a0fc3598"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Verificando la calidad de los primeros 5 chunks ---\n",
            "--- CHUNK 1 (Página: 1) ---\n",
            "GROSS ANATOMY\n",
            "THE BIG PICTURE\n",
            "---------------------------------------------------\n",
            "\n",
            "--- CHUNK 2 (Página: 2) ---\n",
            "Notice\n",
            "Medicine is an ever-changing science. As new research and clinical experience broaden our knowledge, changes in treatment and drug therapy are\n",
            "required. The authors and the publisher of this work have checked with sources believed to be reliable in their efforts to provide information that\n",
            "is complete and generally in accord with the standards accepted at the time of publication. However, in view of the possibility of human error or\n",
            "---------------------------------------------------\n",
            "\n",
            "--- CHUNK 3 (Página: 2) ---\n",
            "changes in medical sciences, neither the authors nor the publisher nor any other party who has been involved in the preparation or publication\n",
            "of this work warrants that the information contained herein is in every respect accurate or complete, and they disclaim all responsibility for any\n",
            "---------------------------------------------------\n",
            "\n",
            "--- CHUNK 4 (Página: 2) ---\n",
            "errors or omissions or for the results obtained from use of the information contained in this work. Readers are encouraged to confirm the infor\u0002mation contained herein with other sources. For example and in particular, readers are advised to check the product information sheet included\n",
            "in the package of each drug they plan to administer to be certain that the information contained in this work is accurate and that changes have\n",
            "---------------------------------------------------\n",
            "\n",
            "--- CHUNK 5 (Página: 2) ---\n",
            "not been made in the recommended dose or in the contraindications for administration. This recommendation is of particular importance in\n",
            "connection with new or infrequently used drugs.\n",
            "---------------------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.0. Se vincula la Base de datos."
      ],
      "metadata": {
        "id": "sGMsf-c3IMSl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Importación de Módulos para Vectorización y Almacenamiento ---\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings #1\n",
        "from langchain_community.vectorstores import Chroma #2\n",
        "\n",
        "# DOCUMENTACIÓN.\n",
        "# 1. Actua como 'traductor semántico':\n",
        "# -- Convierte los fragmentos de texto ('chunks') en números (un vector).\n",
        "# 2. Clase que LangChain utiliza para comunicarse con la DB.\n",
        "\n",
        "# 1. Definir el modelo de embeddings\n",
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "\n",
        "# 2. Crear la base de datos vectorial con los chunks y sus embeddings.\n",
        "# Esta línea tomará cada 'chunk' de 'docs', lo convertirá en un vector.\n",
        "# -- Usando el modelo de embeddings,\n",
        "# -- Lo almacenará en ChromaDB.\n",
        "vector_store = Chroma.from_documents(docs, embeddings, persist_directory=\"chroma_db\")\n",
        "\n",
        "print(\"Base de datos vectorial creada y guardada en el directorio 'Chroma_DB'.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJe34tlnITZ7",
        "outputId": "8cc682ee-51df-42a4-a624-d0a5995549d6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Base de datos vectorial creada y guardada en el directorio 'chroma_db'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.0. Definir el LLM."
      ],
      "metadata": {
        "id": "WlQKb235J3lc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Importación del Módulo del Modelo de Lenguaje (LLM) ---\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# Se elige el modelo.\n",
        "# Se define una temperatura no muy alta. Respuestas precisas, no muy creativas.\n",
        "LLM = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash-latest\",\n",
        "                         temperature=0.8)\n",
        "\n",
        "print(\"LLM configurado exitosamente con el modelo: Gemini-1.5-flash-latest.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Idiy3tBJ-E_",
        "outputId": "f27d48c6-12f6-49b3-fb82-8cc016c48e37"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLM configurado exitosamente con el modelo: Gemini-1.5-flash-latest.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.0. Entorno de consulta."
      ],
      "metadata": {
        "id": "VbsviYUeKOP0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Importación de Componentes para la Construcción de la Cadena RAG ---\n",
        "from langchain.chains import create_retrieval_chain #1\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain #2\n",
        "from langchain_core.prompts import ChatPromptTemplate #3\n",
        "\n",
        "# DOCUMENTACIÓN:\n",
        "# 1. Función para crear la cadena de recuperación.\n",
        "# -- Tomar la pregunta del usuario y usar el 'retriever' para buscar el contexto.\n",
        "# -- Pasa la pregunta y el contexto recuperado a la siguiente cadena en la secuencia.\n",
        "# -- (la 'document_chain') para que genere la respuesta final.\n",
        "# 2. Función específica para manejar documentos.\n",
        "# -- Toma todos los fragmentos recuperados, los junta en un solo bloque.\n",
        "# -- los inserta en la plantilla del prompt junto con la pregunta del usuario.\n",
        "# 3. Clase para crear plantillas de prompts.\n",
        "\n",
        "# \"RETRIEVER\": busca en la base de datos vectorial.\n",
        "retriever = vector_store.as_retriever(\n",
        "    search_type=\"mmr\",\n",
        "    search_kwargs={'k': 5, 'fetch_k': 20}\n",
        "    # 'fetch_k' busca z chunks, 'k' selecciona los x mejores y más diversos.\n",
        ")\n",
        "\n",
        "# Prompt para dar las instrucciones al LLM.\n",
        "# SOLO usará el contexto (los chunks) para responder.\n",
        "# Este primer ejemplo se realizó con documentos sobre Anatomía.\n",
        "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "Actúa como un asistente experto en anatomía humana. Tu tarea es responder la pregunta del usuario. Para ello, analiza y sintetiza la información de TODOS los fragmentos proporcionados en el siguiente contexto.\n",
        "\n",
        "<context>\n",
        "{context}\n",
        "</context>\n",
        "\n",
        "Utiliza la información del contexto para construir una respuesta detallada y completa a esta pregunta: {input}\n",
        "\n",
        "Presta especial atención a detalles específicos como regiones anatómicas, inervaciones, irrigaciones, contenido de los espacios y relaciones espaciales. Si varios fragmentos se complementan, combínalos en una única respuesta coherente.\n",
        "\n",
        "Si, después de analizar todo el contexto, la información necesaria no se encuentra, responde de forma clara y directa: \"La información solicitada no se encuentra en los documentos proporcionados.\"\n",
        "\"\"\")\n",
        "\n",
        "# Combina los documentos recuperados en un solo bloque de texto.\n",
        "document_chain = create_stuff_documents_chain(LLM, prompt)\n",
        "\n",
        "# Une todo: recupera documentos y luego genera la respuesta.\n",
        "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
        "\n",
        "print(\"Cadena RAG creada exitosamente.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ve0GrFs_KSnA",
        "outputId": "291e02a9-d292-492e-a798-be8b75673583"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cadena RAG creada exitosamente.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.0. Hacer preguntas."
      ],
      "metadata": {
        "id": "hYg5bgpcLn_3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pregunta = \"What is the subarachnoid space?\"\n",
        "\n",
        "# Invocamos la cadena con nuestra pregunta.\n",
        "response = retrieval_chain.invoke({\"input\": pregunta})\n",
        "\n",
        "# Imprimimos la respuesta\n",
        "print(\"Respuesta generada:\")\n",
        "print(response[\"answer\"])\n",
        "\n",
        "# Para ver qué chunks usó para responder\n",
        "print(\"\\n--- Contexto Utilizado ---\")\n",
        "for doc in response[\"context\"]:\n",
        "    print(f\"Fuente: {doc.metadata.get('source', 'N/A')}, Página: {doc.metadata.get('page', 'N/A')}\")\n",
        "    print(doc.page_content)\n",
        "    print(\"--------------------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8o8WX2dLsc8",
        "outputId": "0b6c5b6b-e29b-4a75-82d6-237f06f82ccb"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Respuesta generada:\n",
            "El espacio subaracnoideo es la cavidad ubicada entre la aracnoides y la piamadre, dos de las tres meninges que protegen al encéfalo y la médula espinal.  Contiene líquido cefalorraquídeo (LCR), el cual suspende la médula espinal, el cerebro y las raíces nerviosas.  Además, grandes vasos sanguíneos recorren el espacio subaracnoideo.  Una parte del espacio subaracnoideo se conoce como saco dural.  La aracnoides, la capa meníngea intermedia, está unida a la piamadre subyacente por numerosas trabéculas aracnoideas.\n",
            "\n",
            "--- Contexto Utilizado ---\n",
            "Fuente: /content/drive/MyDrive/RAG_MED-001_DB/Gross-Anatomy-The-Big-Picture.pdf, Página: 192\n",
            "period of days or even a week. Enlarging the subdural space is one\n",
            "factor that increases the risk of a subdural hematoma. As the sub\u0002dural space enlarges, the bridging veins that traverse the space\n",
            "travel over a wider distance, causing them to be more vulnerable\n",
            "to tears. As a result, infants (who have smaller brains), the elderly\n",
            "(whose brains atrophy with age), and alcoholics (whose brains\n",
            "atrophy from alcohol use) are at increased risk of developing a\n",
            "--------------------------\n",
            "Fuente: /content/drive/MyDrive/RAG_MED-001_DB/Gross-Anatomy-The-Big-Picture.pdf, Página: 26\n",
            "ARACHNOID MATER\n",
            "The arachnoid mater is the intermediate meningeal layer. It is\n",
            "attached to the underlying pia by numerous arachnoid trabec\u0002ulae. The subarachnoid space is the cavity between the arach\u0002noid and pial layers and contains cerebrospinal fluid (CSF).\n",
            "CSF suspends the spinal cord, brain, and nerve roots. In addi\u0002tion, large blood vessels course within the subarachnoid space.\n",
            "The dural sac is that portion of the subarachnoid space\n",
            "--------------------------\n",
            "Fuente: /content/drive/MyDrive/RAG_MED-001_DB/Gross-Anatomy-The-Big-Picture.pdf, Página: 196\n",
            "■ Parietal lobe. The parietal lobe interprets sensations from the\n",
            "body. The gyrus posterior to the central sulcus, the postcen\u0002tral sulcus, is the primary area for receipt of these sensations.\n",
            "■ Occipital lobe. The occipital lobe is located superior to the\n",
            "tentorium cerebelli, in the posterior cranial fossa, and is pri\u0002marily concerned with vision.\n",
            "■ Temporal lobe. The temporal lobe is located in the middle\n",
            "cranial fossa and is primarily concerned with hearing.\n",
            "--------------------------\n",
            "Fuente: /content/drive/MyDrive/RAG_MED-001_DB/Gross-Anatomy-The-Big-Picture.pdf, Página: 482\n",
            "(CSF) is most likely obtained from which of the following\n",
            "regions?\n",
            "A. Epidural space\n",
            "B. Intervertebral foramen\n",
            "C. Subarachnoid space\n",
            "D. Subdural space\n",
            "E. Subpial space\n",
            "468 SECTION 8 Final Examination\n",
            "--------------------------\n",
            "Fuente: /content/drive/MyDrive/RAG_MED-001_DB/Gross-Anatomy-The-Big-Picture.pdf, Página: 181\n",
            "SECTION 4\n",
            "HEAD\n",
            "--------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Listar modelos de LLM disponibles."
      ],
      "metadata": {
        "id": "eoaJaBjRQVxd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# OPCIONAL:\n",
        "# Verificación de Modelos de IA Disponibles.\n",
        "import google.generativeai as genai #1\n",
        "\n",
        "#1. Permite consultar directamente a la API de Google para obtener una lista\n",
        "#   de todos los modelos de IAG a los que la API Key tiene acceso.\n",
        "\n",
        "for model in genai.list_models():\n",
        "  if 'generateContent' in model.supported_generation_methods:\n",
        "    print(model.name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 774
        },
        "id": "iocFkEnsQZhr",
        "outputId": "a80802e0-d9a6-46d5-9e33-7cf2cf50dac3"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "models/gemini-1.5-pro-latest\n",
            "models/gemini-1.5-pro-002\n",
            "models/gemini-1.5-pro\n",
            "models/gemini-1.5-flash-latest\n",
            "models/gemini-1.5-flash\n",
            "models/gemini-1.5-flash-002\n",
            "models/gemini-1.5-flash-8b\n",
            "models/gemini-1.5-flash-8b-001\n",
            "models/gemini-1.5-flash-8b-latest\n",
            "models/gemini-2.5-pro-preview-03-25\n",
            "models/gemini-2.5-flash-preview-05-20\n",
            "models/gemini-2.5-flash\n",
            "models/gemini-2.5-flash-lite-preview-06-17\n",
            "models/gemini-2.5-pro-preview-05-06\n",
            "models/gemini-2.5-pro-preview-06-05\n",
            "models/gemini-2.5-pro\n",
            "models/gemini-2.0-flash-exp\n",
            "models/gemini-2.0-flash\n",
            "models/gemini-2.0-flash-001\n",
            "models/gemini-2.0-flash-exp-image-generation\n",
            "models/gemini-2.0-flash-lite-001\n",
            "models/gemini-2.0-flash-lite\n",
            "models/gemini-2.0-flash-preview-image-generation\n",
            "models/gemini-2.0-flash-lite-preview-02-05\n",
            "models/gemini-2.0-flash-lite-preview\n",
            "models/gemini-2.0-pro-exp\n",
            "models/gemini-2.0-pro-exp-02-05\n",
            "models/gemini-exp-1206\n",
            "models/gemini-2.0-flash-thinking-exp-01-21\n",
            "models/gemini-2.0-flash-thinking-exp\n",
            "models/gemini-2.0-flash-thinking-exp-1219\n",
            "models/gemini-2.5-flash-preview-tts\n",
            "models/gemini-2.5-pro-preview-tts\n",
            "models/learnlm-2.0-flash-experimental\n",
            "models/gemma-3-1b-it\n",
            "models/gemma-3-4b-it\n",
            "models/gemma-3-12b-it\n",
            "models/gemma-3-27b-it\n",
            "models/gemma-3n-e4b-it\n",
            "models/gemma-3n-e2b-it\n",
            "models/gemini-2.5-flash-lite\n"
          ]
        }
      ]
    }
  ]
}