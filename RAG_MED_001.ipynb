{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNr+PDXC+F/WbxS4Sw5GB91",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Martin-Munive/RAG_MED/blob/main/RAG_MED_001.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.0. InstalaciÃ³n de librerÃ­as.\n"
      ],
      "metadata": {
        "id": "9yfvqgZr-ODF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Core y Modelos de Lenguaje ---\n",
        "!pip install -q -U google-generativeai #1\n",
        "!pip install -q -U langchain #2\n",
        "!pip install -q -U langchain_google_genai #3\n",
        "# --- Carga y Procesamiento de Documentos ---\n",
        "!pip install -q -U pypdfium2 #4\n",
        "# --- Base de Datos Vectorial y Componentes de la Comunidad ---\n",
        "!pip install -q -U chromadb #5\n",
        "!pip install -q -U langchain-community #6\n",
        "\n",
        "## DOCUMENTACIÃ“N:\n",
        "# 1. SDK oficial de Google para interactuar con los modelos de IAG.\n",
        "# 2. Framework principal para flujo del RAG.\n",
        "# 3. Paquete adaptador que permite a LangChain comunicarse con Gemini.\n",
        "# 4. LibrerÃ­a base para la manipulaciÃ³n y procesamiento de PDFs.\n",
        "# -- Permite la extracciÃ³n del texto.\n",
        "# 5. Base de datos vectorial. Almacena los \"chunks\" de texto y sus vectores.\n",
        "# 6. Paquete de las integraciones y componentes.\n",
        "\n",
        "## PENDIENTE\n",
        "# La siguiente librerÃ­a no es necesaria para el funcionamiento del RAG.\n",
        "# Se tiene en consideraciÃ³n para una futura optimizaciÃ³n.\n",
        "# Re-ranking de resultados.\n",
        "# -- Mejora la precisiÃ³n de la fase de recuperaciÃ³n.\n",
        "# !pip install -q -U flashrank\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zlr_Ggfu-gNM",
        "outputId": "f430f5f4-179b-441d-9bfb-777358310361"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m442.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.18 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m313.2/313.2 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m933.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m815.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m19.6/19.6 MB\u001b[0m \u001b[31m47.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m71.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m103.1/103.1 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m65.6/65.6 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m120.0/120.0 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m201.6/201.6 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.4/71.4 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m93.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m453.1/453.1 kB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.0. ConfiguraciÃ³n de la API-KEY."
      ],
      "metadata": {
        "id": "uPd10oeP_qBJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- ImportaciÃ³n de MÃ³dulos Necesarios ---\n",
        "import google.generativeai as genai #1\n",
        "from google.colab import userdata #2\n",
        "import os #3\n",
        "\n",
        "# DOCUMENTACIÃ“N\n",
        "# 1. SDK de Google: para la configuraciÃ³n directa de la API.\n",
        "# 2. MÃ³dulo de Colab: permite acceder a los \"Secretos\" guardados.\n",
        "# 3. MÃ³dulo del Sistema Operativo: para establecer variables de entorno.\n",
        "\n",
        "# ConfiguraciÃ³n Segura de la API Key de Google\n",
        "# Se gestiona el error durante su verificaciÃ³n.\n",
        "try:\n",
        "    GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "    os.environ['GOOGLE_API_KEY'] = GOOGLE_API_KEY\n",
        "    genai.configure(api_key=GOOGLE_API_KEY)\n",
        "    print(\"API Key configurada exitosamente desde los secretos de Colab.\")\n",
        "except userdata.SecretNotFoundError:\n",
        "    print(\"Error: El secreto 'GOOGLE_API_KEY' no fue encontrado.\")\n",
        "    print(\"Por favor, sigue las instrucciones para aÃ±adir el secreto usando el Ã­cono de la llave (ğŸ”‘) a la izquierda.\")\n",
        "except Exception as e:\n",
        "    print(f\"OcurriÃ³ un error inesperado: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jcZOkVLn_wYZ",
        "outputId": "da447f9b-5786-42e9-a92f-31715e8e8c74"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API Key configurada exitosamente desde los secretos de Colab.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.0. VinculaciÃ³n de Google Drive."
      ],
      "metadata": {
        "id": "ya0RF0KqFGzX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- ImportaciÃ³n del MÃ³dulo de Drive ---\n",
        "from google.colab import drive\n",
        "\n",
        "# --- EjecuciÃ³n del Montaje ---\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "print(\"Conectado exitosamente a Google Drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_y-Y6Va1Fft9",
        "outputId": "3f6cb0fd-0bab-4fa1-ce7b-1dffe6d0716c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n",
            "Conectado exitosamente a Google Drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.0. Cargar y dividir los PDFs."
      ],
      "metadata": {
        "id": "jGpMN3ImHCpH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- ImportaciÃ³n de MÃ³dulos para Carga y Procesamiento de Texto ---\n",
        "import os #1\n",
        "from langchain_community.document_loaders import PyPDFium2Loader #2\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter #3\n",
        "\n",
        "# DOCUMENTACIÃ“N:\n",
        "# 1. Proporciona funciones para interactuar con el sistema operativo.\n",
        "# 2. Cargador de directorios de PDF. Escanear la carpeta y buscar archivos.\n",
        "# 3. Clase para dividir texto. Toma el texto de los PDF y los fragmenta.\n",
        "\n",
        "# Ruta a la carpeta en Google Drive\n",
        "DATA_PATH = \"/content/drive/MyDrive/RAG_MED-001_DB\"\n",
        "print(f\"Buscando archivos PDF en la carpeta: {DATA_PATH}\")\n",
        "\n",
        "# Carga para mÃºltiples PDFs.\n",
        "all_documents = []\n",
        "\n",
        "# Itera sobre cada archivo en el directorio.\n",
        "# Verifica si el archivo es un pdf.\n",
        "# Se construye la ruta del archivo.\n",
        "for filename in os.listdir(DATA_PATH):\n",
        "    if filename.lower().endswith(\".pdf\"):\n",
        "        file_path = os.path.join(DATA_PATH, filename)\n",
        "        print(f\"Procesando archivo: {file_path}\")\n",
        "        # Se cargan los archivos PDF y se gestiona el error.\n",
        "        try:\n",
        "            loader = PyPDFium2Loader(file_path)\n",
        "            all_documents.extend(loader.load())\n",
        "        except Exception as e:\n",
        "            print(f\"Error al procesar el archivo {filename}: {e}\")\n",
        "\n",
        "\n",
        "# DivisiÃ³n de los documentos cargados.\n",
        "# Se gestiona el error y se renombra la lista.\n",
        "if not all_documents:\n",
        "    print(\"Â¡Advertencia! No se cargaron documentos. Verifica la ruta y el contenido de la carpeta.\")\n",
        "else:\n",
        "    documents = all_documents\n",
        "\n",
        "# Dividimos los documentos en chunks.\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
        "    docs = text_splitter.split_documents(documents)\n",
        "\n",
        "    print(f\"\\nSe han cargado un total de {len(documents)} pÃ¡gina(s) desde los archivos PDF.\")\n",
        "    print(f\"Se han dividido en {len(docs)} fragmentos (chunks).\")\n",
        "    if docs:\n",
        "        print(\"Ejemplo del primer chunk:\", docs[0].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HYobG3gHSfs",
        "outputId": "db47bcfd-2714-42ae-a069-e36458dd686e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Buscando archivos PDF en la carpeta: /content/drive/MyDrive/RAG_MED-001_DB\n",
            "Procesando archivo: /content/drive/MyDrive/RAG_MED-001_DB/Gross-Anatomy-The-Big-Picture.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pypdfium2/_helpers/textpage.py:80: UserWarning: get_text_range() call with default params will be implicitly redirected to get_text_bounded()\n",
            "  warnings.warn(\"get_text_range() call with default params will be implicitly redirected to get_text_bounded()\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Se han cargado un total de 513 pÃ¡gina(s) desde los archivos PDF.\n",
            "Se han dividido en 2844 fragmentos (chunks).\n",
            "Ejemplo del primer chunk: GROSS ANATOMY\n",
            "THE BIG PICTURE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1. VerificaciÃ³n de los chunks.\n",
        "\n"
      ],
      "metadata": {
        "id": "ayo7o64TZ8PF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imprime los primeros 5 chunks para inspeccionar su calidad.\n",
        "print(\"--- Verificando la calidad de los primeros 5 chunks ---\")\n",
        "for i, doc in enumerate(docs[:5]):\n",
        "    print(f\"--- CHUNK {i+1} (PÃ¡gina: {doc.metadata.get('page', 'N/A')}) ---\")\n",
        "    print(doc.page_content)\n",
        "    print(\"---------------------------------------------------\\n\")\n",
        "# Muestra exactamente cÃ³mo se estÃ¡ dividiendo el PDF.\n",
        "# Si hay tÃ­tulos de secciÃ³n, texto desordenado o contenido sin sentido.\n",
        "# --- Permite identificar problemas es la extracciÃ³n del PDF.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2pZ8WGcaEde",
        "outputId": "01fed6ce-58f7-413f-95cc-d071a0fc3598"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Verificando la calidad de los primeros 5 chunks ---\n",
            "--- CHUNK 1 (PÃ¡gina: 1) ---\n",
            "GROSS ANATOMY\n",
            "THE BIG PICTURE\n",
            "---------------------------------------------------\n",
            "\n",
            "--- CHUNK 2 (PÃ¡gina: 2) ---\n",
            "Notice\n",
            "Medicine is an ever-changing science. As new research and clinical experience broaden our knowledge, changes in treatment and drug therapy are\n",
            "required. The authors and the publisher of this work have checked with sources believed to be reliable in their efforts to provide information that\n",
            "is complete and generally in accord with the standards accepted at the time of publication. However, in view of the possibility of human error or\n",
            "---------------------------------------------------\n",
            "\n",
            "--- CHUNK 3 (PÃ¡gina: 2) ---\n",
            "changes in medical sciences, neither the authors nor the publisher nor any other party who has been involved in the preparation or publication\n",
            "of this work warrants that the information contained herein is in every respect accurate or complete, and they disclaim all responsibility for any\n",
            "---------------------------------------------------\n",
            "\n",
            "--- CHUNK 4 (PÃ¡gina: 2) ---\n",
            "errors or omissions or for the results obtained from use of the information contained in this work. Readers are encouraged to confirm the infor\u0002mation contained herein with other sources. For example and in particular, readers are advised to check the product information sheet included\n",
            "in the package of each drug they plan to administer to be certain that the information contained in this work is accurate and that changes have\n",
            "---------------------------------------------------\n",
            "\n",
            "--- CHUNK 5 (PÃ¡gina: 2) ---\n",
            "not been made in the recommended dose or in the contraindications for administration. This recommendation is of particular importance in\n",
            "connection with new or infrequently used drugs.\n",
            "---------------------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.0. Se vincula la Base de datos."
      ],
      "metadata": {
        "id": "sGMsf-c3IMSl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- ImportaciÃ³n de MÃ³dulos para VectorizaciÃ³n y Almacenamiento ---\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings #1\n",
        "from langchain_community.vectorstores import Chroma #2\n",
        "\n",
        "# DOCUMENTACIÃ“N.\n",
        "# 1. Actua como 'traductor semÃ¡ntico':\n",
        "# -- Convierte los fragmentos de texto ('chunks') en nÃºmeros (un vector).\n",
        "# 2. Clase que LangChain utiliza para comunicarse con la DB.\n",
        "\n",
        "# 1. Definir el modelo de embeddings\n",
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "\n",
        "# 2. Crear la base de datos vectorial con los chunks y sus embeddings.\n",
        "# Esta lÃ­nea tomarÃ¡ cada 'chunk' de 'docs', lo convertirÃ¡ en un vector.\n",
        "# -- Usando el modelo de embeddings,\n",
        "# -- Lo almacenarÃ¡ en ChromaDB.\n",
        "vector_store = Chroma.from_documents(docs, embeddings, persist_directory=\"chroma_db\")\n",
        "\n",
        "print(\"Base de datos vectorial creada y guardada en el directorio 'Chroma_DB'.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJe34tlnITZ7",
        "outputId": "8cc682ee-51df-42a4-a624-d0a5995549d6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Base de datos vectorial creada y guardada en el directorio 'chroma_db'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.0. Definir el LLM."
      ],
      "metadata": {
        "id": "WlQKb235J3lc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- ImportaciÃ³n del MÃ³dulo del Modelo de Lenguaje (LLM) ---\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# Se elige el modelo.\n",
        "# Se define una temperatura no muy alta. Respuestas precisas, no muy creativas.\n",
        "LLM = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash-latest\",\n",
        "                         temperature=0.8)\n",
        "\n",
        "print(\"LLM configurado exitosamente con el modelo: Gemini-1.5-flash-latest.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Idiy3tBJ-E_",
        "outputId": "f27d48c6-12f6-49b3-fb82-8cc016c48e37"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLM configurado exitosamente con el modelo: Gemini-1.5-flash-latest.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.0. Entorno de consulta."
      ],
      "metadata": {
        "id": "VbsviYUeKOP0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- ImportaciÃ³n de Componentes para la ConstrucciÃ³n de la Cadena RAG ---\n",
        "from langchain.chains import create_retrieval_chain #1\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain #2\n",
        "from langchain_core.prompts import ChatPromptTemplate #3\n",
        "\n",
        "# DOCUMENTACIÃ“N:\n",
        "# 1. FunciÃ³n para crear la cadena de recuperaciÃ³n.\n",
        "# -- Tomar la pregunta del usuario y usar el 'retriever' para buscar el contexto.\n",
        "# -- Pasa la pregunta y el contexto recuperado a la siguiente cadena en la secuencia.\n",
        "# -- (la 'document_chain') para que genere la respuesta final.\n",
        "# 2. FunciÃ³n especÃ­fica para manejar documentos.\n",
        "# -- Toma todos los fragmentos recuperados, los junta en un solo bloque.\n",
        "# -- los inserta en la plantilla del prompt junto con la pregunta del usuario.\n",
        "# 3. Clase para crear plantillas de prompts.\n",
        "\n",
        "# \"RETRIEVER\": busca en la base de datos vectorial.\n",
        "retriever = vector_store.as_retriever(\n",
        "    search_type=\"mmr\",\n",
        "    search_kwargs={'k': 5, 'fetch_k': 20}\n",
        "    # 'fetch_k' busca z chunks, 'k' selecciona los x mejores y mÃ¡s diversos.\n",
        ")\n",
        "\n",
        "# Prompt para dar las instrucciones al LLM.\n",
        "# SOLO usarÃ¡ el contexto (los chunks) para responder.\n",
        "# Este primer ejemplo se realizÃ³ con documentos sobre AnatomÃ­a.\n",
        "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "ActÃºa como un asistente experto en anatomÃ­a humana. Tu tarea es responder la pregunta del usuario. Para ello, analiza y sintetiza la informaciÃ³n de TODOS los fragmentos proporcionados en el siguiente contexto.\n",
        "\n",
        "<context>\n",
        "{context}\n",
        "</context>\n",
        "\n",
        "Utiliza la informaciÃ³n del contexto para construir una respuesta detallada y completa a esta pregunta: {input}\n",
        "\n",
        "Presta especial atenciÃ³n a detalles especÃ­ficos como regiones anatÃ³micas, inervaciones, irrigaciones, contenido de los espacios y relaciones espaciales. Si varios fragmentos se complementan, combÃ­nalos en una Ãºnica respuesta coherente.\n",
        "\n",
        "Si, despuÃ©s de analizar todo el contexto, la informaciÃ³n necesaria no se encuentra, responde de forma clara y directa: \"La informaciÃ³n solicitada no se encuentra en los documentos proporcionados.\"\n",
        "\"\"\")\n",
        "\n",
        "# Combina los documentos recuperados en un solo bloque de texto.\n",
        "document_chain = create_stuff_documents_chain(LLM, prompt)\n",
        "\n",
        "# Une todo: recupera documentos y luego genera la respuesta.\n",
        "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
        "\n",
        "print(\"Cadena RAG creada exitosamente.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ve0GrFs_KSnA",
        "outputId": "291e02a9-d292-492e-a798-be8b75673583"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cadena RAG creada exitosamente.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.0. Hacer preguntas."
      ],
      "metadata": {
        "id": "hYg5bgpcLn_3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pregunta = \"What is the subarachnoid space?\"\n",
        "\n",
        "# Invocamos la cadena con nuestra pregunta.\n",
        "response = retrieval_chain.invoke({\"input\": pregunta})\n",
        "\n",
        "# Imprimimos la respuesta\n",
        "print(\"Respuesta generada:\")\n",
        "print(response[\"answer\"])\n",
        "\n",
        "# Para ver quÃ© chunks usÃ³ para responder\n",
        "print(\"\\n--- Contexto Utilizado ---\")\n",
        "for doc in response[\"context\"]:\n",
        "    print(f\"Fuente: {doc.metadata.get('source', 'N/A')}, PÃ¡gina: {doc.metadata.get('page', 'N/A')}\")\n",
        "    print(doc.page_content)\n",
        "    print(\"--------------------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8o8WX2dLsc8",
        "outputId": "0b6c5b6b-e29b-4a75-82d6-237f06f82ccb"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Respuesta generada:\n",
            "El espacio subaracnoideo es la cavidad ubicada entre la aracnoides y la piamadre, dos de las tres meninges que protegen al encÃ©falo y la mÃ©dula espinal.  Contiene lÃ­quido cefalorraquÃ­deo (LCR), el cual suspende la mÃ©dula espinal, el cerebro y las raÃ­ces nerviosas.  AdemÃ¡s, grandes vasos sanguÃ­neos recorren el espacio subaracnoideo.  Una parte del espacio subaracnoideo se conoce como saco dural.  La aracnoides, la capa menÃ­ngea intermedia, estÃ¡ unida a la piamadre subyacente por numerosas trabÃ©culas aracnoideas.\n",
            "\n",
            "--- Contexto Utilizado ---\n",
            "Fuente: /content/drive/MyDrive/RAG_MED-001_DB/Gross-Anatomy-The-Big-Picture.pdf, PÃ¡gina: 192\n",
            "period of days or even a week. Enlarging the subdural space is one\n",
            "factor that increases the risk of a subdural hematoma. As the sub\u0002dural space enlarges, the bridging veins that traverse the space\n",
            "travel over a wider distance, causing them to be more vulnerable\n",
            "to tears. As a result, infants (who have smaller brains), the elderly\n",
            "(whose brains atrophy with age), and alcoholics (whose brains\n",
            "atrophy from alcohol use) are at increased risk of developing a\n",
            "--------------------------\n",
            "Fuente: /content/drive/MyDrive/RAG_MED-001_DB/Gross-Anatomy-The-Big-Picture.pdf, PÃ¡gina: 26\n",
            "ARACHNOID MATER\n",
            "The arachnoid mater is the intermediate meningeal layer. It is\n",
            "attached to the underlying pia by numerous arachnoid trabec\u0002ulae. The subarachnoid space is the cavity between the arach\u0002noid and pial layers and contains cerebrospinal fluid (CSF).\n",
            "CSF suspends the spinal cord, brain, and nerve roots. In addi\u0002tion, large blood vessels course within the subarachnoid space.\n",
            "The dural sac is that portion of the subarachnoid space\n",
            "--------------------------\n",
            "Fuente: /content/drive/MyDrive/RAG_MED-001_DB/Gross-Anatomy-The-Big-Picture.pdf, PÃ¡gina: 196\n",
            "â–  Parietal lobe. The parietal lobe interprets sensations from the\n",
            "body. The gyrus posterior to the central sulcus, the postcen\u0002tral sulcus, is the primary area for receipt of these sensations.\n",
            "â–  Occipital lobe. The occipital lobe is located superior to the\n",
            "tentorium cerebelli, in the posterior cranial fossa, and is pri\u0002marily concerned with vision.\n",
            "â–  Temporal lobe. The temporal lobe is located in the middle\n",
            "cranial fossa and is primarily concerned with hearing.\n",
            "--------------------------\n",
            "Fuente: /content/drive/MyDrive/RAG_MED-001_DB/Gross-Anatomy-The-Big-Picture.pdf, PÃ¡gina: 482\n",
            "(CSF) is most likely obtained from which of the following\n",
            "regions?\n",
            "A. Epidural space\n",
            "B. Intervertebral foramen\n",
            "C. Subarachnoid space\n",
            "D. Subdural space\n",
            "E. Subpial space\n",
            "468 SECTION 8 Final Examination\n",
            "--------------------------\n",
            "Fuente: /content/drive/MyDrive/RAG_MED-001_DB/Gross-Anatomy-The-Big-Picture.pdf, PÃ¡gina: 181\n",
            "SECTION 4\n",
            "HEAD\n",
            "--------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Listar modelos de LLM disponibles."
      ],
      "metadata": {
        "id": "eoaJaBjRQVxd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# OPCIONAL:\n",
        "# VerificaciÃ³n de Modelos de IA Disponibles.\n",
        "import google.generativeai as genai #1\n",
        "\n",
        "#1. Permite consultar directamente a la API de Google para obtener una lista\n",
        "#   de todos los modelos de IAG a los que la API Key tiene acceso.\n",
        "\n",
        "for model in genai.list_models():\n",
        "  if 'generateContent' in model.supported_generation_methods:\n",
        "    print(model.name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 774
        },
        "id": "iocFkEnsQZhr",
        "outputId": "a80802e0-d9a6-46d5-9e33-7cf2cf50dac3"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "models/gemini-1.5-pro-latest\n",
            "models/gemini-1.5-pro-002\n",
            "models/gemini-1.5-pro\n",
            "models/gemini-1.5-flash-latest\n",
            "models/gemini-1.5-flash\n",
            "models/gemini-1.5-flash-002\n",
            "models/gemini-1.5-flash-8b\n",
            "models/gemini-1.5-flash-8b-001\n",
            "models/gemini-1.5-flash-8b-latest\n",
            "models/gemini-2.5-pro-preview-03-25\n",
            "models/gemini-2.5-flash-preview-05-20\n",
            "models/gemini-2.5-flash\n",
            "models/gemini-2.5-flash-lite-preview-06-17\n",
            "models/gemini-2.5-pro-preview-05-06\n",
            "models/gemini-2.5-pro-preview-06-05\n",
            "models/gemini-2.5-pro\n",
            "models/gemini-2.0-flash-exp\n",
            "models/gemini-2.0-flash\n",
            "models/gemini-2.0-flash-001\n",
            "models/gemini-2.0-flash-exp-image-generation\n",
            "models/gemini-2.0-flash-lite-001\n",
            "models/gemini-2.0-flash-lite\n",
            "models/gemini-2.0-flash-preview-image-generation\n",
            "models/gemini-2.0-flash-lite-preview-02-05\n",
            "models/gemini-2.0-flash-lite-preview\n",
            "models/gemini-2.0-pro-exp\n",
            "models/gemini-2.0-pro-exp-02-05\n",
            "models/gemini-exp-1206\n",
            "models/gemini-2.0-flash-thinking-exp-01-21\n",
            "models/gemini-2.0-flash-thinking-exp\n",
            "models/gemini-2.0-flash-thinking-exp-1219\n",
            "models/gemini-2.5-flash-preview-tts\n",
            "models/gemini-2.5-pro-preview-tts\n",
            "models/learnlm-2.0-flash-experimental\n",
            "models/gemma-3-1b-it\n",
            "models/gemma-3-4b-it\n",
            "models/gemma-3-12b-it\n",
            "models/gemma-3-27b-it\n",
            "models/gemma-3n-e4b-it\n",
            "models/gemma-3n-e2b-it\n",
            "models/gemini-2.5-flash-lite\n"
          ]
        }
      ]
    }
  ]
}